library(tidyverse)
library(janitor)
library(readxl)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
library(rpart)
library(rpart.plot)
rpart.df <- rpart(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, method="anova",data=train_data)
rpart.plot(rpart.df,type=3)
plotcp(rpart.df)#choose cp=0.012
#prune
pfit<- prune(rpart.df, cp=0.012)
rpart.plot(pfit,type=3,main="Pruned Regression tree for happiness score")
pred <- predict(pfit,test_data)
mean((pred - test_data$life_ladder)^2)  #0.35
pfit<- prune(rpart.df, cp=0.014)
rpart.plot(pfit,type=3,main="Pruned Regression tree for happiness score")
pred <- predict(pfit,test_data)
mean((pred - test_data$life_ladder)^2)
library(randomForest)
set.seed(2)
bag_df <- randomForest(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, data = train_data,mtry = 9, importance =TRUE)
importance(bag_df)
varImpPlot(bag_df)
varImp(bag_df)
plot(varImp(object=bag_df),
main="bagging - Variable Importance")
pred_df <- predict(bag_df,test_data)
mean((pred_df - test_data$life_ladder)^2)
library(caret)
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
plot(model, plotType = "level")
plot(varImp(object=model),
main="GBM - Variable Importance")
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2) #0.22
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 4,
trControl=train_control)
print(model_rf)
plot(model_rf)
plot(varImp(object=model_rf),
main="RF - Variable Importance")
varImp(object=model_rf)
print(model_rf)
varImp(object=model_rf)
varImp(model_rf)
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2)
importance(model_rf)
print(model_rf)
varImp(model_rf)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 4,
trControl=train_control,
importance="Ture")
print(model_rf)
varImp(model_rf)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 4,
trControl=train_control,
importance="Ture")
?train
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
trControl=train_control
)
plot(varImp(object=model_rf),
main="RF - Variable Importance")
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
trControl=train_control,
importance=Ture)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
importance=TRUE,
method='rf',
trControl=train_control
)
print(model_rf)
varImp(model_rf)
plot(varImp(object=model_rf),
main="RF - Variable Importance")
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
library(tidyverse)
library(janitor)
library(readxl)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
df.class<-df
df.class$life_ladder <- bin(df$life_ladder, nbins = 3, method = "content")
?bin
??bin
?gsub
library(tidyverse)
library(janitor)
library(readxl)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
library(tidyverse)
library(janitor)
library(readxl)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
rpart.df <- rpart(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, method="anova",data=train_data)
rpart.plot(rpart.df,type=3)
plotcp(rpart.df)#choose cp=0.014
#prune
pfit<- prune(rpart.df, cp=0.014)
rpart.plot(pfit,type=3,main="Pruned Regression tree for happiness score")
pred <- predict(pfit,test_data)
mean((pred - test_data$life_ladder)^2)  #0.315
set.seed(2)
bag_df <- randomForest(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, data = train_data,mtry = 9, importance =TRUE)
importance(bag_df) #Health and social support and GPA are most important
pred_df <- predict(bag_df,test_data)
mean((pred_df - test_data$life_ladder)^2) #test error=0.156
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
varImp(object=model)
plot(varImp(object=model),
main="GBM - Variable Importance") #healthy,GDP,social support
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2) #0.20
varImp(object=model)
print(model)
varImp(object=model)
caret::varImp(object=model)
?vatimp
??VarImp
caret::varImp(object=model)
caret::varImp(model)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
varImp(model)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
importance=TRUE,
method='rf',
trControl=train_control
)
print(model_rf)
varImp(model_rf)
plot(varImp(object=model_rf),
main="RF - Variable Importance") #healthy,social,democratic
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2) #0.151
print(model)
varImp(model)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid
)
# summarize results
print(model)
varImp(model)
plot(model)
varImp(object=model)
varImp(model,value="gcb")
varImp(model,numTrees=NULL)
varImp(model,relative.influence=NULL)
varImp(model,relative.influence=FALSE)
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(4,6,8),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
metric = "ROC",
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
varImp(model,relative.influence=FALSE)
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(4,6,8),
n.minobsinnode = c(10))
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
varImp(model,relative.influence=FALSE)
print(model)
plot(varImp(object=model),
main="GBM - Variable Importance") #healthy,GDP,social support
varImp(model,relative.influence=FALSE)
varImp(model)
set.seed(2)
model <- train(life_ladder~.,
train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid)
print(model)
varImp(model)
caret::varImp(model)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
print(model)
summary(model)
importance(model)
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2) #0.20
pred_2
pred_2 <- predict(model_rf,test_data)
pred_1 <- predict(model,test_data)
pred_1
library(tidyverse)
library(janitor)
library(readxl)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
df<-read_excel("WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
library(rpart.plot)
install.packages("rpart.plot")
install.packages("rworldmap")
install.packages("spam")
?cv.glmnet
??cv.glmnet
library(glmnet)
library(dplyr)
library(tidyverse)
library(readxl)
library(janitor)
library(rworldmap)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(glmnet)
df<-read_excel("WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df1=df %>%
drop_na()
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
pls_model2=train(life_ladder~.,data= df,
method = "pls",
preProcess=c("center", "scale"),
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10), tuneLength = 9 )
min(pls_model2$results$RMSE)^2
varImp(pls_model2)
pls_model2$bestTune
plot(pls_model2,main ="PLSR")
# 0.329472
set.seed(1)
bagmars_model=train(life_ladder~.,data= df,
method = "bagEarth",
preProcess=c("center", "scale"),
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
set.seed(1)
bagmars_model=train(life_ladder~.,data= df,
method = "bagEarth",
preProcess=c("center", "scale"),
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
set.seed(1)
bagmars_model=train(life_ladder~.,data= df,
method = "bagEarth",
preProcess=c("center", "scale"),
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
set.seed(1)
gamboost_model=train(life_ladder~.,data= df,
method = "gamboost",
preProcess=c("center", "scale"),
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
set.seed(1)
gamboost_model=train(life_ladder~.,data= df,
method = "gamboost",
preProcess=c("center", "scale"),
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
set.seed(1)
gamboost_model=train(life_ladder~.,data= df,
method = "gamboost",
preProcess=c("center", "scale"),
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
min(gamboost_model$results[3])^2
gamboost_model$bestTune
plot(gamboost_model, main="Boosting GAM")
#0.2826903
set.seed(1)
knnmod <- train(life_ladder ~ ., data = df,
method = "knn",
preProcess=c("center", "scale"),
tuneLength = 12,
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10))
min(knnmod$results$RMSE)^2
knnmod$bestTune
plot(knnmod,main="KNN")
#0.1811445
set.seed(1)
svmmod <- train(life_ladder ~ ., data = df,
method = "svmLinear3",
preProcess=c("center", "scale"),
trControl = trainControl(method = "LGOCV",verboseIter=T,number=10))
View(df)
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
View(df)
