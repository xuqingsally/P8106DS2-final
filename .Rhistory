load("C:/Users/Qing/Desktop/Columbia University/course/Spring_2018/p8106DS2/loan.RData")
index <- createDataPartition(train$Loan_Status, p=0.75, list=FALSE)
trainSet <- train[index,]
testSet <- train[-index,]
rpart.plot(model)
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2)
pred_1 <- predict(model,df)
mean((pred_1 - test_data$life_ladder)^2)
mean((pred_1 - df$life_ladder)^2) #0.05
View(train_data)
model <- train(train_data[,-1],
train_data[,1],
trControl=train_control,
method="gbm",
tuneGrid=grid,
metric = "ROC",
preProcess = c("center","scale"),
verbose = FALSE)
model <- train(life_ladder_.,
df=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
metric = "ROC",
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
set.seed(2)
model <- train(life_ladder~.,
df=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
metric = "ROC",
preProcess = c("center","scale"),
verbose = FALSE)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
metric = "ROC",
preProcess = c("center","scale"),
verbose = FALSE)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
print(model)
plot(model, plotType = "level")
varImp(object=model)
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2)
?trainControl
model_rf<-train(train_data[,-1],
train_data[,1],
method='rf',
tuneLength = 4,
metric = "ROC",
trControl=ctrl)
model_rf<-train(train_data[,-1],
train_data[,1],
method='rf',
tuneLength = 4,
metric = "ROC",
trControl=train_control)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 4,
metric = "ROC",
trControl=train_control)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 4,
trControl=train_control)
plot(model_rf, plotType = "level")
print(model_rf)
plot(model_rf, plotType = "level")
plot(model_rf)
varImp(object=model)
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 10,
trControl=train_control)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 7,
trControl=train_control)
print(model_rf)
plot(model_rf)
varImp(object=model_rf)
plot(varImp(object=model_rf),
main="RF - Variable Importance")
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2)
library(tidyverse)
library(janitor)
library(readxl)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
library(rpart)
library(rpart.plot)
rpart.df <- rpart(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, method="anova",data=train_data)
rpart.plot(rpart.df,type=3)
plotcp(rpart.df)#choose cp=0.012
#prune
pfit<- prune(rpart.df, cp=0.012)
rpart.plot(pfit,type=3,main="Pruned Regression tree for happiness score")
pred <- predict(pfit,test_data)
mean((pred - test_data$life_ladder)^2)  #0.35
pfit<- prune(rpart.df, cp=0.014)
rpart.plot(pfit,type=3,main="Pruned Regression tree for happiness score")
pred <- predict(pfit,test_data)
mean((pred - test_data$life_ladder)^2)
library(randomForest)
set.seed(2)
bag_df <- randomForest(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, data = train_data,mtry = 9, importance =TRUE)
importance(bag_df)
varImpPlot(bag_df)
varImp(bag_df)
plot(varImp(object=bag_df),
main="bagging - Variable Importance")
pred_df <- predict(bag_df,test_data)
mean((pred_df - test_data$life_ladder)^2)
library(caret)
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
plot(model, plotType = "level")
plot(varImp(object=model),
main="GBM - Variable Importance")
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2) #0.22
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 4,
trControl=train_control)
print(model_rf)
plot(model_rf)
plot(varImp(object=model_rf),
main="RF - Variable Importance")
varImp(object=model_rf)
print(model_rf)
varImp(object=model_rf)
varImp(model_rf)
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2)
importance(model_rf)
print(model_rf)
varImp(model_rf)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 4,
trControl=train_control,
importance="Ture")
print(model_rf)
varImp(model_rf)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
tuneLength = 4,
trControl=train_control,
importance="Ture")
?train
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
trControl=train_control
)
plot(varImp(object=model_rf),
main="RF - Variable Importance")
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
method='rf',
trControl=train_control,
importance=Ture)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
importance=TRUE,
method='rf',
trControl=train_control
)
print(model_rf)
varImp(model_rf)
plot(varImp(object=model_rf),
main="RF - Variable Importance")
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
library(tidyverse)
library(janitor)
library(readxl)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
df.class<-df
df.class$life_ladder <- bin(df$life_ladder, nbins = 3, method = "content")
?bin
??bin
?gsub
library(tidyverse)
library(janitor)
library(readxl)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
library(tidyverse)
library(janitor)
library(readxl)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
rpart.df <- rpart(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, method="anova",data=train_data)
rpart.plot(rpart.df,type=3)
plotcp(rpart.df)#choose cp=0.014
#prune
pfit<- prune(rpart.df, cp=0.014)
rpart.plot(pfit,type=3,main="Pruned Regression tree for happiness score")
pred <- predict(pfit,test_data)
mean((pred - test_data$life_ladder)^2)  #0.315
set.seed(2)
bag_df <- randomForest(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, data = train_data,mtry = 9, importance =TRUE)
importance(bag_df) #Health and social support and GPA are most important
pred_df <- predict(bag_df,test_data)
mean((pred_df - test_data$life_ladder)^2) #test error=0.156
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
varImp(object=model)
plot(varImp(object=model),
main="GBM - Variable Importance") #healthy,GDP,social support
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2) #0.20
varImp(object=model)
print(model)
varImp(object=model)
caret::varImp(object=model)
?vatimp
??VarImp
caret::varImp(object=model)
caret::varImp(model)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
varImp(model)
set.seed(2)
model_rf<-train(life_ladder~.,
train_data,
importance=TRUE,
method='rf',
trControl=train_control
)
print(model_rf)
varImp(model_rf)
plot(varImp(object=model_rf),
main="RF - Variable Importance") #healthy,social,democratic
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2) #0.151
print(model)
varImp(model)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid
)
# summarize results
print(model)
varImp(model)
plot(model)
varImp(object=model)
varImp(model,value="gcb")
varImp(model,numTrees=NULL)
varImp(model,relative.influence=NULL)
varImp(model,relative.influence=FALSE)
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(4,6,8),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
metric = "ROC",
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
# summarize results
print(model)
varImp(model,relative.influence=FALSE)
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(4,6,8),
n.minobsinnode = c(10))
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
importance=T,
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
set.seed(2)
model <- train(life_ladder~.,
data=train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
varImp(model,relative.influence=FALSE)
print(model)
plot(varImp(object=model),
main="GBM - Variable Importance") #healthy,GDP,social support
varImp(model,relative.influence=FALSE)
varImp(model)
set.seed(2)
model <- train(life_ladder~.,
train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid)
print(model)
varImp(model)
caret::varImp(model)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
df<-read_excel("../Copy of WHR2018Chapter2OnlineData.xls") %>%
clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df<-df[,-1:-2]
df<-df %>%
drop_na()
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
shrinkage=c(0.01,0.05,0.1,0.5),
interaction.depth=c(2,4),
n.minobsinnode = c(10))
# train the model
set.seed(2)
model <- train(life_ladder~.,
train_data,
trControl=train_control,
method="gbm",
tuneGrid=grid,
preProcess = c("center","scale"),
verbose = FALSE)
print(model)
summary(model)
importance(model)
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2) #0.20
pred_2
pred_2 <- predict(model_rf,test_data)
pred_1 <- predict(model,test_data)
pred_1
