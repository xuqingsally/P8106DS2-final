---
title: "final"
author: "Qing Xu"
date: "April 21, 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Exploratory Analysis

```{r}
library(dplyr)
library(tidyverse)
library(readxl)
library(janitor)
library(rworldmap)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(glmnet)
df<-read_excel("WHR2018Chapter2OnlineData.xls") %>%
  clean_names()
df<-df[,1:14]
df<-df[,-10:-11]
df1=df %>% 
  drop_na()
df<-df[,-1:-2]
df<-df %>%
  drop_na()
```

##Correlation Analysis

```{r}
library(corrplot)
df_corr = df1%>%
  dplyr::select(-country,-year)
ladder_corr <- cor( df_corr, use = "complete", method = "pearson")
corrplot(ladder_corr)
```

We could see that GDP per capita, health life expectancy at birth, delivery quality and social support are highly correlated with life ladder. And perception of corruption has negative correlation with life ladder. But confidence in national government also has negative correlation with life ladder, though it is close to zero, which may indicate that people in countries with questioning about government may be happier.

##World Map of Happiness

```{r}
df_map = df1%>%
  dplyr::filter(year == 2016)%>%
  dplyr::select(country,life_ladder) 
 
colnames(df_map)<-c("country","value")
correction16<-c("Guinea"="Equatorial Guinea", "Serbia"="Republic of Serbia",  "Tanzania"="United Republic of Tanzania","United States"="United States of America","United States"="United States Virgin Islands","Russia"="Russian Federation","Venezuela"="Venezuela, Bolivarian Republic of","Bolivia"="Bolivia (Plurinational State of)","South Korea"="Republic of Korea" )
for(i in names(correction16)){
  df_map[df_map$country==i,"country"]<-correction16[i]
}

d <- data.frame(
  country=df_map$country,
  value=df_map$value)

n <- joinCountryData2Map(d, joinCode="NAME", nameJoinColumn="country")
mapCountryData(n, nameColumnToPlot="value", mapTitle="World Map for Hapiness in 2016",colourPalette="terrain")
```

The happiest countries are countries from Northern Europe, Australia, and Canada. Europe, America and Oceania are at a high level of happiness. Countries in Asia and Africa own a lower level of happiness, especially some countries at Africa.

##Cluster Analysis
```{r}
df_cluster = df1%>%
  dplyr::filter(year == 2016)%>%
  dplyr::select(-year,-country,-life_ladder) 
df_cluster = scale(df_cluster) 
d_cluster = data.frame(
  country = df_map$country,
  data = df_cluster
) 

data.dist=dist(d_cluster)
plot(hclust(data.dist), labels=d_cluster$country, 
     main="Complete Linkage")
df_rank =
df_map%>%
  arrange(desc(value))
```
 
Cluster analysis is conducted using all variables expect life ladder. There are two main clusters and most countries in the left cluster has a higher GDP than the right one.


#Regression
```{r}
x = model.matrix(life_ladder~.,df)[,-1]
y = df$life_ladder
```

#Ridge regression

```{r ridge}
set.seed(1)
#10 fold cv
cv.ridge = cv.glmnet(x,y,alpha=0, type.measure = "mse")
plot(cv.ridge, main="Ridge regression CV MSE-lambda")
cv.ridge$lambda.min
min(cv.ridge$cvm)
#0.3312722
```

Through 10 fold cross validation, we found the minimal cross validation MSE for ridge regression is 0.3313 when lambda is 0.09845.

#Lasso regression

```{r lasso}
set.seed(1)
#10 fold cv
cv.lasso = cv.glmnet(x,y,alpha=1, type.measure = "mse")
plot(cv.lasso, main="Lasso regression CV MSE-lambda")
opt_lambdalasso = cv.lasso$lambda.min
min(cv.lasso$cvm)
#0.1324718
```

Through 10 fold cross validation, we found the minimal cross validation MSE for lasso regression is 0.1325 when lambda is 0.002328.

#Elastic net model

```{r elastic net model lgocv}
set.seed(1)
glmnetmodel2 <- train(
  life_ladder~., data = df, method = "glmnet",
  preProcess=c("center", "scale"),
  trControl = trainControl("LGOCV", number = 10)
  ,tuneLength = 9
  )
ggplot(glmnetmodel2)+
  theme_light()+
  labs(title="Elastic-net LGOCV")
glmnetmodel2$bestTune
min(glmnetmodel2$results$RMSE)^2
#0.3294714
```

Through leave group out cross validation(LGOCV), we found the minimal cross validation MSE for elastic net model is 0.3295 when alpha is 0.8875 and lambda is 0.002843475.

#Principal component regression

```{r pcr LGOCV}
set.seed(1)
pcrmodel1 <- train(
  life_ladder~., data = df, method = "pcr",
  preProcess=c("center", "scale"),
  trControl = trainControl("LGOCV", number = 10)
  ,tuneLength = 9
  )
plot(pcrmodel1,main="PCR")

pcrmodel1$bestTune
min(pcrmodel1$results$RMSE)^2
#biplot(pcrmodel$finalModel)
#0.3304539
```

Through LGOCV, we found the minimal cross validation MSE for PCR is 0.3305 when the model includes 8 principal components.

#Partial least square regression

```{r pls lgocv}
set.seed(1)
pls_model2=train(life_ladder~.,data= df,
                 method = "pls",
                 preProcess=c("center", "scale"),
                 trControl = trainControl(method = "LGOCV",verboseIter=T,number=10), tuneLength = 9 )
min(pls_model2$results$RMSE)^2
pls_model2$bestTune
plot(pls_model2,main ="PLSR")
# 0.329472
```

Through LGOCV, we found the minimal cross validation MSE for PLSR is 0.3295 when the model includes 6 PLS components. PLSR has less component and shows lower CV MSE compared with PCR. This is because PLSR chooses components with regard to both maximal variance and the correlation with the response, whereas PCR only selects principal components based on maximal variance.

#Random forest

```{r random forest regression tree^}
set.seed(1)
rf_model=train(life_ladder~.,data= df,
                 method = "rf",
                 preProcess=c("center", "scale"),
                 trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
min(rf_model$results$RMSE)^2
rf_model$bestTune
plot(rf_model, main ="Random forest")
# 0.1718519
```

Through LGOCV, we found the minimal cross validation MSE for random forest is 0.1719 when each tree has 3 predictors.

#Gradient Boosting Machine

```{r gbm ^}
gbmgrid=expand.grid(interaction.depth = c(1,3,5), 
                        n.trees = (2:30)*150, 
                        shrinkage = c(0.01,0.001),
                        n.minobsinnode = 10)
set.seed(1)
boost_model=train(life_ladder~.,data= df,
                 method = "gbm",
                 preProcess=c("center", "scale"),
                 trControl = trainControl(method = "LGOCV",verboseIter=T,number=10), tuneGrid=gbmgrid )
min(boost_model$results$RMSE)^2
ggplot(boost_model)+
  theme(panel.background = element_rect(fill = "white", colour = "grey50"))+
  labs(title="GBM LGOCV")
boost_model$bestTune
# 0.1903925
```

Through LGOCV, we found the minimal cross validation MSE for GBM is 0.1904 when the model has 4500 trees, each tree has 5 splits, and the shrinkage parameter is 0.01.

#Bagging trees

```{r bagged tree}
set.seed(1)
bag_model=train(life_ladder~.,data= df,
                 method = "treebag",
                 preProcess=c("center", "scale"),
                 trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ) )
bag_model$results[2]^2
# 0.2945061
```
Through LGOCV, we found the minimal cross validation MSE for bagging trees is 0.2945.

#Bagging MARS
```{r bagging mars multivariate adaptive regression splines}
set.seed(1)
bagmars_model=train(life_ladder~.,data= df,
                 method = "bagEarth",
                 preProcess=c("center", "scale"),
                 trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
min(bagmars_model$results$RMSE)^2
bagmars_model$bestTune
plot(bagmars_model, main="Bagging MARS")
#0.2750501
```

Through LGOCV, the minimal cross validation MSE for bagging MARS is 0.2751 with nprune=13 and degree=1.

#Boosted GAM

```{r boosted GAM}
set.seed(1)
gamboost_model=train(life_ladder~.,data= df,
                 method = "gamboost",
                 preProcess=c("center", "scale"),
                 trControl = trainControl(method = "LGOCV",verboseIter=T,number=10 ), tuneLength = 5 )
min(gamboost_model$results[3])^2
gamboost_model$bestTune
plot(gamboost_model, main="Boosting GAM")
#0.2826903
```

Through LGOCV, the minimal cross validation MSE for Boosted GAM is 0.2827.

#K-nearest neighbours

```{r knn ^}
set.seed(1)
knnmod <- train(life_ladder ~ ., data = df,
             method = "knn",
             preProcess=c("center", "scale"),
             tuneLength = 12,
             trControl = trainControl(method = "LGOCV",verboseIter=T,number=10))
min(knnmod$results$RMSE)^2
knnmod$bestTune
plot(knnmod,main="KNN")

#0.1811445
```

Through LGOCV, the minimal cross validation MSE for KNN is 0.1811 when K is 5.

#Support vector machine(Linear)
```{r linear svm}
set.seed(1)
svmgrid=expand.grid(cost=(1:10)*0.1, Loss=c("L1","L2"))
svmmod <- train(life_ladder ~ ., data = df,
             method = "svmLinear3",
             preProcess=c("center", "scale"),
             trControl = trainControl(method = "LGOCV",verboseIter=T,number=10), tuneGrid=svmgrid)
min(svmmod$results$RMSE)^2
plot(svmmod, main="SVM")
svmmod$bestTune
ggplot(svmmod)+
  theme_light()+
  labs(title="SVM")
#0.3297174
```

Through LGOCV, the minimal cross validation MSE for SVM is 0.3297 when cost is 0.6 and when L1 loss is used.

#Model comparison
For the 10 models above, we use resamples function to compare their performance.
```{r resampling}
resamp <- resamples(list( plsr = pls_model2, pcr = pcrmodel1,glmnet=glmnetmodel2,GBM=boost_model,gamboost=gamboost_model,randomForest=rf_model,bagEarth=bagmars_model,treebag=bag_model, knn=knnmod, svm=svmmod))
bwplot(resamp, metric = "RMSE")
densityplot(resamp, metric = "RMSE")
splom(resamp, metric = "RMSE")

diff1=diff(resamp)
summary(diff1)
```

From resampling results, we can see that random forest has the best prediction of happiness score among the 10 methods, followed by KNN and GBM. The medium performance models include boosted GAM, bagging tree, and bagging MARS. Lastly, PCR, PLSR, elastic net, and linear SVM have similar MSEs, which are higher than other models. Their poor performance is due to the linear restrictions, hence the true model is probably nonlinear. From the density plot, it is also seen that the boosted GAM has the smallest variability.

After fitting the regression model with the whole dataset, we also tried splitting the data into training set and test set to fit regression and classification models.

#regression tree
```{r}
set.seed(1)
index <- createDataPartition(df$life_ladder, p=0.75, list=FALSE)
train_data <- df[index,]
test_data <- df[-index,]
```
Separate data set into train set and validation set in 3:1.  

```{r}
rpart.df <- rpart(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, method="anova",data=train_data)
rpart.plot(rpart.df,type=3)
plotcp(rpart.df)#choose cp=0.014

#prune
pfit<- prune(rpart.df, cp=0.014)
rpart.plot(pfit,type=3,main="Pruned Regression tree for happiness score")
pred <- predict(pfit,test_data)
mean((pred - test_data$life_ladder)^2)  #0.315

```
Choose size=7 and cp=0.014,we get a prune tree with 7 terminal nodes and their means.  
The test error is 0.315.  

#bagging 
```{r}
set.seed(2)
bag_df <- randomForest(life_ladder~log_gdp_per_capita+social_support+healthy_life_expectancy_at_birth+freedom_to_make_life_choices+generosity+perceptions_of_corruption+confidence_in_national_government+democratic_quality+delivery_quality, data = train_data,mtry = 9, importance =TRUE)
importance(bag_df) #Health and social support and GPA are most important
pred_df <- predict(bag_df,test_data)
mean((pred_df - test_data$life_ladder)^2) #test error=0.156
```
Using bagging and we found that the three most important variables are healthy life expectancy at birth, GDP and social support. These three factors all have positive association with happiness scores.  
The test error is 0.156.  

#boosting
```{r}
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(n.trees=c(50,100,500,1000), #tuning parameter
                    shrinkage=c(0.01,0.05,0.1,0.5),
                    interaction.depth=c(2,4),
                    n.minobsinnode = c(10)) 
# train the model
set.seed(2)
model <- train(life_ladder~., 
               train_data, 
               trControl=train_control, 
               method="gbm", 
               tuneGrid=grid,
               preProcess = c("center","scale"),
               verbose = FALSE)
# summarize results
print(model)
summary(model) #healthy, gdp,social
pred_1 <- predict(model,test_data)
mean((pred_1 - test_data$life_ladder)^2) #0.20
```
Using boosting for scaled and center data, we found that the three most important variables are healthy life expectancy at birth,GDP and social support. These three factors all have positive association with happiness scores.   
The test error is 0.20.  

#rf
```{r}
set.seed(2)
model_rf<-train(life_ladder~.,
                train_data,
                importance=TRUE,
                method='rf', 
                trControl=train_control
                )
print(model_rf)
varImp(model_rf)
plot(varImp(object=model_rf),
     main="RF - Variable Importance") #healthy,social,democratic
pred_2 <- predict(model_rf,test_data)
mean((pred_2 - test_data$life_ladder)^2) #0.151
```
Using random forest and we found that the three most important variables are healthy life expectancy at birth,social support and democratic quality. These three factors all have positive association with happiness scores.   
The test error is 0.151.  

During these ensembel methods for regression tree, random forest method has smallest test error so that it is the best method. The three most important variables for happiness scores are similar. Healthy life expectancy at birth is the most important variable and the larger average equivalent number of years of full health that a newborn could expect to live,the higher happiness score will the people have. Higher GDP, complete social support and high democratic quality will bring people happiness.  
We can see that money of the country is not the most important factor of happiness but it still is a key point.Health and social support play a important part in people's happiness life. 